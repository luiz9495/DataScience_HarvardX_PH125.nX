---
title: 'Data Science: Machine Learning - HarvardX: PH125.8x'
author: 'Luiz Cunha'
date: '2019-08-15'
output: html_notebook
---

# Section 3: Linear Regression for Prediction, Smoothing, and Working with Matrices Overview

##  Overview

In the **Linear Regression for Prediction, Smoothing, and Working with Matrices Overview** section, you will learn why linear regression is a useful baseline approach but is often insufficiently flexible for more complex analyses, how to smooth noisy data, and how to use matrices for machine learning.

After completing this section, you will be able to:

* Use **linear regression for prediction** as a baseline approach.
* Use **logistic regression** for categorical data.
* Detect trends in noisy data using **smoothing** (also known as **curve fitting** or **low pass filtering**).
* Convert predictors to **matrices** and outcomes to **vectors** when all predictors are numeric (or can be converted to numerics in a meaningful way).
* Perform basic **matrix algebra** calculations.

This section has three parts: **linear regression for prediction**, **smoothing** and **working with matrices**. 


## 3.1 Linear Regression for Prediction

### 3.1.1 Linear Regression for Prediction

**Key points**

* Linear regression can be considered a machine learning algorithm. Although it can be too rigid to be useful, it works rather well for some challenges. It also serves as a baseline approach: if you cant beat it with a more complex approach, you probably want to stick to linear regression. 

**Code**
```{r}
library(tidyverse)
library(caret)
library(HistData)
set.seed(1983)

galton_heights <- GaltonFamilies %>%
  filter(gender == "male") %>%
  group_by(family) %>%
  sample_n(1) %>%
  ungroup() %>%
  select(father, childHeight) %>%
  rename(son = childHeight)

y <- galton_heights$son
test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE)
train_set <- galton_heights %>% slice(-test_index)
test_set <- galton_heights %>% slice(test_index)

m <- mean(train_set$son)

# squared loss
mean((m - test_set$son)^2)

# fit linear regression model
fit <- lm(son ~ father, data = train_set)
fit$coef
y_hat <- fit$coef[1] + fit$coef[2]*test_set$father
mean((y_hat - test_set$son)^2)
```

### 3.1.2 The predict() Function

**Key points**

* The **predict** function takes a fitted object from functions such as **lm** or **glm** and a data frame with the new predictors for which to predict. We can use predict like this: *y_hat <- predict(fit, test_set)*
* **predict** is a generic function in R that calls other functions depending on what kind of object it receives. To learn about the specifics, you can read the help files using code like this: *?predict.lm* or *?predict.glm*

**Code**
```{r}
y_hat <- predict(fit, test_set)
mean((y_hat - test_set$son)^2)

# read help files
?predict.lm
?predict.glm
```

### 3.1.3 Comprehension Check: Linear Regression

#### Question 1
We will build 100 linear models using the data above and calculate the mean and standard deviation of the combined models.  

* First, set the seed to 1 again (make sure to use sample.kind="Rounding" if your R is version 3.6 or later).  
* Then, within a replicate loop, 
  + (1) partition the dataset into test and training sets of equal size using dat$y to generate your indices, 
  + (2) train a linear model predicting y from x, 
  * (3) generate predictions on the test set, and 
  * (4) calculate the RMSE of that model. 
* Then, report the mean and standard deviation (SD) of the RMSEs from all 100 models.

Create a data set using the following code:
```{r}
options(digits=3)
set.seed(1) 
# set.seed(1, sample.kind="Rounding") if using R 3.6 or later
n <- 100
Sigma <- 9 * matrix(c(1.0, 0.5, 0.5, 1.0), 2, 2)
dat1 <- MASS::mvrnorm(n = 100, c(69, 69), Sigma) %>%
      data.frame() %>% setNames(c("x", "y"))
```

```{r}
set.seed(1) 

RMSE <- replicate(100, {
    test_index <- createDataPartition(dat1$y, times = 1, p = 0.5, list = FALSE)
    train <- dat1 %>% slice(-test_index)
    test <- dat1 %>% slice(test_index)
    mod <- lm(y ~ x, data=train)
    y_hat <- predict(mod, test)
    sqrt(mean((y_hat - test$y)^2))
})
mean(RMSE)
sd(RMSE)
```


#### Question 2
Now we will repeat the exercise above but using larger datasets.  
Write a function that takes a size n, then:

* (1) builds a dataset using the code provided in Q1 but with n observations instead of 100 and without the set.seed(1), 
* (2) runs the replicate loop that you wrote to answer Q1, which builds 100 linear models and returns a vector of RMSEs, and 
* (3) calculates the mean and standard deviation.
* Set the seed to 1 (if using R 3.6 or later, use the argument sample.kind="Rounding") and then 
* use sapply or map to apply your new function to n <- c(100, 500, 1000, 5000, 10000).

Hint: You only need to set the seed once before running your function; do not set a seed within your function. Also be sure to use sapply or map as you will get different answers running the simulations individually due to setting the seed.

```{r}
options(digits=3)
set.seed(1)

n <- c(100, 500, 1000, 5000, 10000)
res2 <- sapply(n, function(n) {
  Sigma <- 9 * matrix(c(1.0, 0.5, 0.5, 1.0), 2, 2)
  dat_n <- MASS::mvrnorm(n, c(69, 69), Sigma) %>%
    data.frame() %>% setNames(c("x", "y"))

  RMSE_n <- replicate(100, {
    test_index <- createDataPartition(dat_n$y, times = 1, 
                                      p = 0.5, list = FALSE)
    train <- dat_n %>% slice(-test_index)
    test <- dat_n %>% slice(test_index)
    mod <- lm(y ~ x, data = train)
    y_hat <- predict(mod, test)
    sqrt(mean((y_hat - test$y)^2))
    })
  c(mu = mean(RMSE_n), sigma = sd(RMSE_n))
})

res2
```

#### Question 3
Q: What happens to the RMSE as the size of the dataset becomes larger?  
A: On average, the RMSE does not change much as n gets larger, but the variability of the RMSE decreases. 

#### Question 4
Now repeat the exercise from Q1, this time making the correlation between x and y larger, as in the following code:

Note what happens to RMSE - set the seed to 1 as before.

```{r}
set.seed(1)
n <- 100
Sigma <- 9*matrix(c(1.0, 0.95, 0.95, 1.0), 2, 2)
dat4 <- MASS::mvrnorm(n = 100, c(69, 69), Sigma) %>%
	data.frame() %>% setNames(c("x", "y"))

set.seed(1)
RMSE <- replicate(100, {
    test_index <- createDataPartition(dat4$y, times = 1, p = 0.5, list = FALSE)
    train <- dat4 %>% slice(-test_index)
    test <- dat4 %>% slice(test_index)
    fit <- lm(y ~ x, data=train)
    y_hat <- predict(fit, test)
    sqrt(mean((y_hat - test$y)^2))
})
c(mu = mean(RMSE), sigma= sd(RMSE))
```

#### Question 5
Q: Which of the following best explains why the RMSE in question 4 is so much lower than the RMSE in question 1?
A: When we increase the correlation between x and y, x has more predictive power and thus provides a better estimate of y.

#### Question 6
Create a data set using the following code.

```{r}
set.seed(1)
Sigma <- matrix(c(1.0, 0.75, 0.75, 0.75, 1.0, 0.25, 0.75, 0.25, 1.0), 3, 3)
dat6 <- MASS::mvrnorm(n = 100, c(0, 0, 0), Sigma) %>%
	data.frame() %>% setNames(c("y", "x_1", "x_2"))
cor(dat6)
```

```{r}
library(tidyverse)
library(caret)
set.seed(1)
test_index <- createDataPartition(dat6$y, times = 1, p = 0.5, list = FALSE)
train <- dat6 %>% slice(-test_index)
test <- dat6 %>% slice(test_index)

fit <- lm(y ~ x_1, data=train)
y_hat <- predict(fit, test)
sqrt(mean((y_hat - test$y)^2))

fit <- lm(y ~ x_2, data=train)
y_hat <- predict(fit, test)
sqrt(mean((y_hat - test$y)^2))

fit <- lm(y ~ x_1+x_2, data=train)
y_hat <- predict(fit, test)
sqrt(mean((y_hat - test$y)^2))
```

Note that y is correlated with both x_1 and x_2 but the two predictors are independent of each other, as seen by cor(dat).

Set the seed to 1, then use the caret package to partition into a test and training set of equal size. Compare the RMSE when using just x_1, just x_2 and both x_1 and x_2. Train a linear model for each.

Q: Which of the three models performs the best (has the lowest RMSE)?  
A: fit <- lm(y ~ x_1+x_2, data=train)

#### Question 8
Repeat the exercise from Q6 but now create an example in which x_1 and x_2 are highly correlated.

```{r}
set.seed(1)
Sigma <- matrix(c(1.0, 0.75, 0.75, 0.75, 1.0, 0.95, 0.75, 0.95, 1.0), 3, 3)
dat8 <- MASS::mvrnorm(n = 100, c(0, 0, 0), Sigma) %>%
	data.frame() %>% setNames(c("y", "x_1", "x_2"))
```

Set the seed to 1, then use the caret package to partition into a test and training set of equal size. Compare the RMSE when using just x_1, just x_2, and both x_1 and x_2.

```{r}
library(tidyverse)
library(caret)
set.seed(1)
test_index <- createDataPartition(dat8$y, times = 1, p = 0.5, list = FALSE)
train <- dat8 %>% slice(-test_index)
test <- dat8 %>% slice(test_index)

fit <- lm(y ~ x_1, data=train)
y_hat <- predict(fit, test)
sqrt(mean((y_hat - test$y)^2))

fit <- lm(y ~ x_2, data=train)
y_hat <- predict(fit, test)
sqrt(mean((y_hat - test$y)^2))

fit <- lm(y ~ x_1+x_2, data=train)
y_hat <- predict(fit, test)
sqrt(mean((y_hat - test$y)^2))
```

Q: Compare the results from Q6 and Q8. What can you conclude?  
A: Adding extra predictors can improve RMSE substantially, but not when the added predictors are highly correlated with other predictors.


### 3.1.4 Regression for a Categorical Outcome

**Key points**

* The regression approach can be extended to categorical data. For example, we can try regression to estimate the conditional probability:

$$p(x) = Pr(Y=1 \mid X=x) = \beta_0 + \beta_1 x$$

* Once we have estimates $beta_0$ and $beta_1$, we can obtain an actual prediction p(x). Then we can define a specific decision rule to form a prediction.

**Code**
```{r}
library(dslabs)
data("heights")
y <- heights$height

set.seed(2) #if you are using R 3.5 or earlier
#set.seed(2, sample.kind = "Rounding") #if you are using R 3.6 or later

test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE)
train_set <- heights %>% slice(-test_index)
test_set <- heights %>% slice(test_index)

train_set %>% 
  filter(round(height)==66) %>%
  summarize(y_hat = mean(sex=="Female"))

heights %>% 
  mutate(x = round(height)) %>%
  group_by(x) %>%
  filter(n() >= 10) %>%
  summarize(prop = mean(sex == "Female")) %>%
  ggplot(aes(x, prop)) +
  geom_point()

lm_fit <- mutate(train_set, y = as.numeric(sex == "Female")) %>% lm(y ~ height, data = .)
p_hat <- predict(lm_fit, test_set)
y_hat <- ifelse(p_hat > 0.5, "Female", "Male") %>% factor()
confusionMatrix(y_hat, test_set$sex)$overall["Accuracy"]
```

### 3.1.5 Logistic Regression

**Key points**

* **Logistic regression** is an extension of linear regression that assures that the estimate of conditional probability $Pr(Y=1 \mid X=x)$ is between 0 and 1. This approach makes use of the logistic transformation: 

$$g(p) = \log\frac{p}{1-p}$$

* With logistic regression, we model the conditional probability directly with:

$$g(Pr(Y=1 \mid X=x)) = \beta_0 + \beta_1 x$$

* Note that with this model, we can no longer use least squares. Instead we compute the **maximum likelihood estimate (MLE)**.
* In R, we can fit the logistic regression model with the function **glm (generalized linear models)**: *eg. glm(formula..., data=..., family="binomial")*
* If we want to compute the conditional probabilities, we want **type="response"** since the default is to return the logistic transformed values: *eg. predict(glm_fitted..., newdata=..., type = "response")*  

**Code**
```{r}
heights %>% 
  mutate(x = round(height)) %>%
  group_by(x) %>%
  filter(n() >= 10) %>%
  summarize(prop = mean(sex == "Female")) %>%
  ggplot(aes(x, prop)) +
  geom_point() + 
  geom_abline(intercept = lm_fit$coef[1], slope = lm_fit$coef[2])
range(p_hat)

# fit logistic regression model
glm_fit <- train_set %>% 
  mutate(y = as.numeric(sex == "Female")) %>%
  glm(y ~ height, data=., family = "binomial")
p_hat_logit <- predict(glm_fit, newdata = test_set, type = "response")
y_hat_logit <- ifelse(p_hat_logit > 0.5, "Female", "Male") %>% factor
confusionMatrix(y_hat_logit, test_set$sex)[["Accuracy"]]
```


### 3.1.6 Case Study: "2" or "7"

**Key points**

* In this case study we apply logistic regression to classify whether a digit is two or seven. We are interested in estimating a conditional probability that depends on two variables:

$$g(p(x_1, x_2)) = g(Pr(Y=1 \mid X_1=x_1, X_2=x_2)) = \beta_0 + \beta_1 x$$

* Through this case, we know that logistic regression forces our estimates to be a **plane** and our boundary to be a **line**. This implies that a logistic regression approach has no chance of capturing the **non-linear** nature of the true $p(x_1,x_2)$. Therefore, we need other more flexible methods that permit other shapes.

**Code**
```{r}
mnist <- read_mnist()
is <- mnist_27$index_train[c(which.min(mnist_27$train$x_1),        
                                                                which.max(mnist_27$train$x_1))]
titles <- c("smallest","largest")
tmp <- lapply(1:2, function(i){
    expand.grid(Row=1:28, Column=1:28) %>%
        mutate(label=titles[i],
               value = mnist$train$images[is[i],])
})
tmp <- Reduce(rbind, tmp)
tmp %>% ggplot(aes(Row, Column, fill=value)) +
    geom_raster() +
    scale_y_reverse() +
    scale_fill_gradient(low="white", high="black") +
    facet_grid(.~label) +
    geom_vline(xintercept = 14.5) +
    geom_hline(yintercept = 14.5)

data("mnist_27")
mnist_27$train %>% ggplot(aes(x_1, x_2, color = y)) + geom_point()

is <- mnist_27$index_train[c(which.min(mnist_27$train$x_2),        
                             which.max(mnist_27$train$x_2))]
titles <- c("smallest","largest")
tmp <- lapply(1:2, function(i){
    expand.grid(Row=1:28, Column=1:28) %>%
        mutate(label=titles[i],
               value = mnist$train$images[is[i],])
})
tmp <- Reduce(rbind, tmp)
tmp %>% ggplot(aes(Row, Column, fill=value)) +
    geom_raster() +
    scale_y_reverse() +
    scale_fill_gradient(low="white", high="black") +
    facet_grid(.~label) +
    geom_vline(xintercept = 14.5) +
    geom_hline(yintercept = 14.5)

fit_glm <- glm(y ~ x_1 + x_2, data=mnist_27$train, family = "binomial")
p_hat_glm <- predict(fit_glm, mnist_27$test)
y_hat_glm <- factor(ifelse(p_hat_glm > 0.5, 7, 2))
confusionMatrix(data = y_hat_glm, reference = mnist_27$test$y)$overall["Accuracy"]

mnist_27$true_p %>% ggplot(aes(x_1, x_2, fill=p)) +
    geom_raster()
mnist_27$true_p %>% ggplot(aes(x_1, x_2, z=p, fill=p)) +
    geom_raster() +
    scale_fill_gradientn(colors=c("#F8766D","white","#00BFC4")) +
    stat_contour(breaks=c(0.5), color="black") 

p_hat <- predict(fit, newdata = mnist_27$true_p)
mnist_27$true_p %>%
    mutate(p_hat = p_hat) %>%
    ggplot(aes(x_1, x_2,  z=p_hat, fill=p_hat)) +
    geom_raster() +
    scale_fill_gradientn(colors=c("#F8766D","white","#00BFC4")) +
    stat_contour(breaks=c(0.5),color="black") 

p_hat <- predict(fit, newdata = mnist_27$true_p)
mnist_27$true_p %>%
    mutate(p_hat = p_hat) %>%
    ggplot() +
    stat_contour(aes(x_1, x_2, z=p_hat), breaks=c(0.5), color="black") +
    geom_point(mapping = aes(x_1, x_2, color=y), data = mnist_27$test)
```

#### 3.1.7 Comprehension Check: Logistic Regression

Define a dataset using the following code:

```{r}
set.seed(2) #if you are using R 3.5 or earlier
#set.seed(2, sample.kind="Rounding") #if you are using R 3.6 or later
make_data <- function(n = 1000, p = 0.5, 
				mu_0 = 0, mu_1 = 2, 
				sigma_0 = 1,  sigma_1 = 1){

y <- rbinom(n, 1, p)
f_0 <- rnorm(n, mu_0, sigma_0)
f_1 <- rnorm(n, mu_1, sigma_1)
x <- ifelse(y == 1, f_1, f_0)
  
test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE)

list(train = data.frame(x = x, y = as.factor(y)) %>% slice(-test_index),
	test = data.frame(x = x, y = as.factor(y)) %>% slice(test_index))
}
dat <- make_data()
```

Note that we have defined a variable x that is predictive of a binary outcome y: 
```{r}
dat$train %>% ggplot(aes(x, color = y)) + geom_density()
```

Set the seed to 1, then use the make_data function defined above to generate 25 different datasets with mu_1 <- seq(0, 3, len=25). Perform logistic regression on each of the 25 different datasets (predict 1 if p>0.5) and plot accuracy (res in the figures) vs mu_1 (delta in the figures).

```{r}
set.seed(1)

# generate 25 datasets
delta <- seq(0, 3, len=25)

# perform the process 25 tims
res <- sapply(delta, function(d){

  # create the dataset
  dat <- make_data(mu_1=d)

  # perform logistic regression fit
  glm_fit <- dat$train %>% 
    glm(y ~ x, data=., family = "binomial")
  
  # predict
  # NB!!! new data is test including xs and ys!!
  p_hat_logit <- predict(glm_fit, newdata = dat$test, type = "response")
  y_hat_logit <- ifelse(p_hat_logit > 0.5, 1, 0) %>% factor(levels = c(0, 1))
  mean(y_hat_logit == dat$test$y)
})

# plot the accuracy as a function of delta (mu)
qplot(delta, res)
```



## 3.2 Smoothing

### 3.2.1 Introduction to Smoothing

**Key points**

* **Smoothing** is a very powerful technique used all across data analysis. It is designed to **detect trend*s** in the presence of noisy data in cases in which the shape of the trend is unknown. 
* The concepts behind smoothing techniques are extremely useful in machine learning because conditional expectations/probabilities can be thought of as **trends** of unknown shapes that we need to estimate in the presence of uncertainty.

**Code**

```{r}
data("polls_2008")
qplot(day, margin, data = polls_2008)
```


### 3.2.2 Bin Smoothing and Kernels

**Key points**

The general idea of smoothing is to **group data points into strata** in which the value of $f(x)$ can be assumed to be constant. We can make this assumption because we think $f(x)$ changes slowly and, as a result, $f(x)$ is almost constant in small windows of time. 
This assumption implies that a good estimate for $f(x)$ is the average of the {Y_i} values in the window. The estimate is:

$$\hat{f}(x_0) = \frac{1}{N_0}\sum_{i\in A_0}Y_i$$
In smoothing, we call the size of the interval $\mid x-x_0\mid$ satisfying the particular condition the **window size**, **bandwidth** or **span**.

**Code**
```{r}
# bin smoothers: avging with continuity by piece
span <- 7 # 7days per week: smoothing over 1 week window
fit <- with(polls_2008, ksmooth(day, margin, x.points = day, kernel="box", bandwidth =span))
polls_2008 %>% mutate(smooth = fit$y) %>%
    ggplot(aes(day, margin)) +
    geom_point(size = 3, alpha = .5, color = "grey") + 
    geom_line(aes(day, smooth), color="red")

# kernel: avging with normal function
span <- 7   # 7days per week: smoothing over 1 week window
fit <- with(polls_2008, ksmooth(day, margin,  x.points = day, kernel="normal", bandwidth = span))
polls_2008 %>% mutate(smooth = fit$y) %>%
  ggplot(aes(day, margin)) +
  geom_point(size = 3, alpha = .5, color = "grey") + 
  geom_line(aes(day, smooth), color="red")
```


### 3.2.3 Local Weighted Regression (loess)

**Key points**

* A limitation of the bin smoothing approach is that we need small windows for the approximately constant assumptions to hold which may lead to imprecise estimates of $f(x)$ . **Local weighted regression (loess)** permits us to consider larger window sizes.
* One important difference between loess and bin smoother is that we assume **the smooth function is locally linear** in a window instead of constant.
* The result of **loess is a smoother fit than bin smoothing** because we use larger sample sizes to estimate our local parameters.

**Code**

```{r}
polls_2008 %>% ggplot(aes(day, margin)) +
  geom_point() + 
  geom_smooth(color="red", span = 0.15, method = 'loess', method.args = list(degree=1))

# NB: geom_smooth defaults to method = 'loess', polynomial approx degree =2, and span = 0.25 (proportion of points to consider for loess weighting)
```

### 3.2.4 Comprehension Check: Smoothing

#### Question 1
In the Wrangling course of this series, PH125.6x, we used the following code to obtain mortality counts for Puerto Rico for 2015-2018:

```{r}
library(tidyverse)
library(lubridate)
library(purrr)
library(pdftools)
    
fn <- system.file("extdata", "RD-Mortality-Report_2015-18-180531.pdf", package="dslabs")
dat <- map_df(str_split(pdf_text(fn), "\n"), function(s){
	s <- str_trim(s)
	header_index <- str_which(s, "2015")[1]
	tmp <- str_split(s[header_index], "\\s+", simplify = TRUE)
	month <- tmp[1]
	header <- tmp[-1]
	tail_index  <- str_which(s, "Total")
	n <- str_count(s, "\\d+")
	out <- c(1:header_index, which(n==1), which(n>=28), tail_index:length(s))
	s[-out] %>%
		str_remove_all("[^\\d\\s]") %>%
		str_trim() %>%
		str_split_fixed("\\s+", n = 6) %>%
		.[,1:5] %>%
		as_data_frame() %>% 
		setNames(c("day", header)) %>%
		mutate(month = month,
			day = as.numeric(day)) %>%
		gather(year, deaths, -c(day, month)) %>%
		mutate(deaths = as.numeric(deaths))
}) %>%
	mutate(month = recode(month, "JAN" = 1, "FEB" = 2, "MAR" = 3, "APR" = 4, "MAY" = 5, "JUN" = 6, 
                          "JUL" = 7, "AGO" = 8, "SEP" = 9, "OCT" = 10, "NOV" = 11, "DEC" = 12)) %>%
	mutate(date = make_date(year, month, day)) %>%
      filter(date <= "2018-05-01")

# Q: Use the loess function to obtain a smooth estimate of the expected number of deaths as a function of date. Plot this resulting smooth function. Make the span about two months long.

# span = 2 months / # date points
span <- 60 / as.numeric(diff(range(dat$date)))

# calculate the loess model fit
fit <- dat %>% mutate(x = as.numeric(date)) %>% loess(deaths ~ x, data = ., span = span, degree = 1)

# predict the loess model output, and plots the line curve on this output
## NB: all this can be done in one shot using ggplot2 geom_smooth() function
dat %>% mutate(smooth = predict(fit, as.numeric(date))) %>%
	ggplot() +
	geom_point(aes(date, deaths)) +
	geom_line(aes(date, smooth), lwd = 2, col = 2)
```

#### Question 2
Work with the same data as in Q1 to plot smooth estimates against day of the year, all on the same plot, but with different colors for each year.

Which code produces the desired plot?
```{r}
dat %>% 
	mutate(smooth = predict(fit, as.numeric(date)), day = yday(date), year = as.character(year(date))) %>%
	ggplot(aes(day, smooth, col = year)) +
	geom_line(lwd = 2)
```

#### Question 3
Suppose we want to predict 2s and 7s in the mnist_27 dataset with just the second covariate. Can we do this? On first inspection it appears the data does not have much predictive power.

In fact, if we fit a regular logistic regression the coefficient for x_2 is not significant!

This can be seen using this code:
```{r}
library(broom)
mnist_27$train %>% glm(y ~ x_2, family = "binomial", data = .) %>% tidy()
```

Plotting a scatterplot here is not useful since y is binary:
```{r}
qplot(x_2, y, data = mnist_27$train)
```

Fit a loess line to the data above and plot the results. What do you observe?
```{r}
mnist_27$train %>% 
	mutate(y = ifelse(y=="7", 1, 0)) %>%
	ggplot(aes(x_2, y)) + 
	geom_smooth(method = "loess")
```


## 3.3 Working with Matrices

### 3.3.1 Matrices

### 3.3.2

### 3.3.3